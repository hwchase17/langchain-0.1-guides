{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376c865a-9f50-417d-a0cf-4c430571d104",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "This notebook covers basic walkthrough of retrieval functionality in LangChain. For more information, see:\n",
    "\n",
    "- [Retrieval Documentation](https://python.langchain.com/docs/modules/data_connection/)\n",
    "\n",
    "- [Advanced Retrieval Types](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "- [QA with RAG Use Case Documentation](https://python.langchain.com/docs/use_cases/question_answering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b594de",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bda46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb65747c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aece40e",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e749d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006a257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792485da",
   "metadata": {},
   "source": [
    "## Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289e53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d2450",
   "metadata": {},
   "source": [
    "## Query Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a66449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "588f5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93cfd3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing a way to run chains over datasets and visualize the outputs. It simplifies dataset uploading and allows users to review and evaluate the results. The LangSmith client makes it easy to pull down a dataset and run a chain over the data points, logging the results to a new project associated with the dataset. Users can assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project. LangSmith also provides evaluators that can be specified when initiating a test run to evaluate the results. Additionally, LangSmith offers annotation queues for manual review and annotation of runs, allowing users to assess subjective qualities and validate auto-evaluated runs.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ba679",
   "metadata": {},
   "source": [
    "## Advanced Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d89b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67347a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "685ccfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/.pyenv/versions/3.11.1/envs/langchain-0.1-guides/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing the following features:\n",
      "\n",
      "1. Visualization of Inputs/Outputs: LangSmith provides a straightforward visualization of the exact inputs and outputs to all LLM calls, allowing users to easily understand them.\n",
      "\n",
      "2. Prompt Editing Playground: LangSmith has a built-in playground where users can modify the prompt of an LLM call and re-run it to observe the resulting changes to the output. This feature supports OpenAI and Anthropic models and works for LLM and Chat Model calls.\n",
      "\n",
      "3. Sequence of Events: LangSmith's tracing feature offers a visualization of the sequence of calls and interactions in complicated chains and agents. This helps users understand the exact sequence of events and the inputs/outputs of each call.\n",
      "\n",
      "4. Latency Monitoring: LangSmith tracks the latency of each step in a chain, allowing users to identify and potentially eliminate the slowest components that may be causing a chain to take longer than expected.\n",
      "\n",
      "5. Token Usage Tracking: LangSmith tracks the total token usage for a chain and the token usage of each step. This helps users identify potentially costly parts of the chain and optimize their token usage.\n",
      "\n",
      "6. Collaborative Debugging: LangSmith provides a \"Share\" button that allows users to share a faulty chain and its LLM runs with colleagues for collaborative debugging. This makes it easier to work together on identifying and resolving issues.\n",
      "\n",
      "7. Collecting Examples: LangSmith includes an \"Add to Dataset\" button for each run, making it easy to add input/output examples to a chosen dataset. Users can edit the examples before adding them, including the expected result. This feature helps in collecting examples of failures and known issues for testing future chain versions.\n",
      "\n",
      "Overall, LangSmith provides various tools and features to facilitate testing and evaluation of LLM applications, improving the reliability and quality of the models.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e992b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
